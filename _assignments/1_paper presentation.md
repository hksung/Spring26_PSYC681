---
type: assignment
date: 2026-01-13T23:59:00+3:30
title: "1. Paper Presentation"
due_event: 
    type: due
    date: 2026-4-16T23:59:00+3:30
    description: '1. Paper Presentation'
---

This is a brief guideline on how to prepare your paper presentation.

The goal of this assignment is to introduce important NLP papers to the class. Students will benefit the most by reading the paper carefully and summarizing its key points. When presenting, focus less on technical implementation details and more on conveying (1) **what was done** and (2) **why the paper is important**.

**Time Limit:** 15–20 minutes (no more than 20 minutes)

When preparing your presentation, consider the following guiding questions:

1. What problem does the paper address?
2. Why is this problem important?
3. What research gap does this paper identify?
4. What research questions or goals guide the study?
5. How did the authors approach the problem?
6. What are the key findings?
7. What does this paper contribute to the field?
8. What are the strengths and limitations of the study?
9. What remains unresolved or could be improved?
10. What are potential future directions?
11. Are there any studies that have built on this work?

*Tip*: If structuring a full presentation is difficult, you may select four questions from the list and answer each in approximately five minutes. For example, I'd recommend the following workflow:

1. First, skim the paper. And then, read the paper more thoroughly.
2. Decide which questions you want to address.
3. Prepare clear answers to those questions.
4. Organize your answers into presentation format (so that you can explain them to your peers).
5. May prepare Q&A section at the end of the presentation.

### Evaluation criteria
Presentations will be evaluated based on the simple criteria (2 points each, 6 points in total):
1. *Clarity*: Are the key ideas communicated clearly?
2. *Depth*: Does the presenter demonstrate understanding of the problem, approach, and significance?
3. *Organization and timing*: Is the presentation well-structured and within the allotted time (1 point each)?

----

## Paper presentation schedule/slides:
- *Note 1*. Presentation slides will be updated following the schedule.
- *Note 2*. Click the `slide` link or the `paper title` to open the paper.

| Date | Presenter        | Authors (Year)            | Title (*click to open paper*) |
| :--- | :--------------- | ------------------------- | ----------------------------- |
| 1/20 | Emily            | Mikolov et al. (2013)     | [Efficient Estimation of Word Representations](https://arxiv.org/pdf/1301.3781) |
| 1/20 | Sindhu           | Pennington et al. (2014)  | [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf) |
| 1/22 | Leona            | Levy et al. (2015)        | [Improving Distributional Similarity in Word Embeddings](https://aclanthology.org/Q15-1016.pdf) |
| 1/27 | Jacob            | Collobert et al. (2011)   | [NLP (Almost) from Scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf#page=6.70) |
| 2/3  | Duvarakanath     | Chen & Manning (2014)     | [A Fast and Accurate Neural Dependency Parser](https://aclanthology.org/D14-1082.pdf) |
| 2/3  | Matthew, Valbona | de Marneffe et al. (2021) | [Universal Dependencies](https://direct.mit.edu/coli/article/47/2/255/98516/Universal-Dependencies) |
| 2/10 | Shubh Sudan      | Sak et al. (2014)         | [LSTM Architectures for Acoustic Modeling](https://www.isca-archive.org/interspeech_2014/sak14_interspeech.pdf) |
| 2/12 | Willow           | Du et al. (2024)          | [Financial Sentiment Analysis](https://dl.acm.org/doi/full/10.1145/3649451) |
| 2/17 | Shubh Sehgal     | Vaswani et al. (2017)     | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) |
| 2/19 | Billy            | Huang et al. (2018)       | [Music Transformer](https://arxiv.org/abs/1809.04281) |
| 2/24 | Pooja            | Devlin (2019)             | [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/pdf/1810.04805) |
| 2/26 | Suruthi          | Smith (2020)              | [Contextual Word Representations](https://arxiv.org/pdf/1902.06006) |
| 3/17 | Joshua           | Chung et al. (2022)       | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) |
| 3/17 | Arshad           | Wang et al. (2022)        | [Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751) |
| 3/19 | Adit, Thejas     | Taguchi & Sproat (2025)   | [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/pdf/2510.07591) |
| 3/26 | Vivian           | Brown et al. (2020)       | [Language Models Are Few-Shot Learners](https://arxiv.org/abs/2005.14165) |
| 3/26 | Nehha            | Hu et al. (2021)          | [LoRA: Low-Rank Adaptation of LLMs](https://arxiv.org/abs/2106.09685) |
| 4/7  | Mandira          | Hendrycks et al. (2021)   | [Measuring Massive Multitask Language Understanding](https://arxiv.org/pdf/2009.03300) |
| 4/7  | Jam, Stephanie   | Liang et al. (2023)       | [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110#page=14.44) |
| 4/9  | Sanjeev          | Yao et al. (2023)         | [ReAct: Reasoning and Acting in LLMs](https://arxiv.org/abs/2210.03629) |
| 4/9  | Dmitrii          | Shick et al. (2023)       | [Toolformer: Teaching LLMs to Use Tools](https://arxiv.org/abs/2302.04761) |
| 4/14 | Saeed            | Wei et al. (2023)         | [Chain-of-Thought Prompting](https://arxiv.org/pdf/2201.11903) |
| 4/14 | Karthik          | Wang et al. (2023)        | [Self-Consistency for Chain-of-Thought Reasoning](https://arxiv.org/pdf/2203.11171) |
| 4/16 | Rahul            | Lightman et al. (2023)    | [Let’s Verify Step by Step](https://arxiv.org/abs/2305.20050) |
| 4/16 | Ishan            | Snell et al. (2024)       | [Scaling LLM Test-Time Compute](https://arxiv.org/abs/2408.03314) |
