---
layout: page
title: Materials
permalink: /materials/
---

## Book

- Jurafsky, D., & Martin, J. H. (2024). *Speech and Language Processing: An introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models* (3rd ed., draft). [Available online](https://web.stanford.edu/~jurafsky/slp3/)

---

## Paper List 

|  # | Authors                   | Title                                                                                                                                                                  |
| -: | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|  1 | Mikolov et al. (2013)     | [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)                                                                        |
|  2 | Pennington et al. (2014)  | [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf)                                                                                 |
|  3 | Levy et al. (2015)        | [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://aclanthology.org/Q15-1016.pdf)                                                 |
|  4 | Collobert et al. (2011)   | [Natural Language Processing (Almost) from Scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf#page=6.70)                                      |
|  5 | Chen & Manning (2014)     | [A Fast and Accurate Dependency Parser using Neural Networks](https://aclanthology.org/D14-1082.pdf)                                                                   |
|  6 | de Marneffe et al. (2021) | [Universal Dependencies](https://direct.mit.edu/coli/article/47/2/255/98516/Universal-Dependencies)                                                                    |
|  7 | Sak et al. (2014)         | [Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling](https://www.isca-archive.org/interspeech_2014/sak14_interspeech.pdf) |
|  8 | Du et al. (2024)          | [Financial Sentiment Analysis: Techniques and Applications](https://dl.acm.org/doi/full/10.1145/3649451)                                                               |
|  9 | Vaswani et al. (2017)     | [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                                                                                          |
| 10 | Huang et al. (2018)       | [Music Transformer](https://arxiv.org/abs/1809.04281)                                                                                                                  |
| 11 | Devlin (2019)             | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)                                                   |
| 12 | Smith (2020)              | [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/pdf/1902.06006)                                                                         |
| 13 | Chung et al. (2022)       | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)                                                                                      |
| 14 | Wang et al. (2022)        | [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)                                                 |
| 15 | Taguchi & Sproat (2025)   | [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/pdf/2510.07591)                                                                                      |
| 16 | Brown et al. (2020)       | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)                                                                                              |
| 17 | Hu et al. (2021)          | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)                                                                                 |
| 18 | Hendrycks et al. (2021)   | [Measuring massive multitask language understanding](https://arxiv.org/pdf/2009.03300)                                                                                                       |
| 19 | Liang et al. (2023)       | [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110#page=14.44)                                                                                  |
| 20 | Yao et al. (2023)         | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)                                                                         |
| 21 | Shick et al. (2023)       | [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)                                                                      |
| 22 | Wei et al. (2023)         | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)                                                              |
| 23 | Wang et al. (2023)        | [Self-consistency improves chain of thought reasoning in language models](https://arxiv.org/pdf/2203.11171)                                                            |
| 24 | Lightman et al. (2023)    | [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)                                                                                                          |
| 25 | Snell et al. (2024)       | [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)                                        |

---

